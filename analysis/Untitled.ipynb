{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Class to preprocess the data, including filtering, rescaling, column removal, rotation, and one-hot encoding operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def preprocess(self, df, remove_background=False, do_intersections=False, min_particles=1):\n",
    "        \"\"\"\n",
    "        Preprocess the DataFrame by calling individual preprocessing subroutines.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to preprocess.\n",
    "        remove_background : bool\n",
    "            If True, remove rows where mc_index is -1.\n",
    "        do_intersections : bool\n",
    "            If True, preprocess intersection related dataframe\n",
    "        min_particles : int\n",
    "            Minimum number of particles per event\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The preprocessed DataFrame.\n",
    "        \"\"\"\n",
    "        if remove_background:\n",
    "            df = df[df['otid'] != -1]\n",
    "        \n",
    "#         # Group by file_number and file_event_number\n",
    "#         grouped = df.groupby(['file_number', 'file_event_number'])\n",
    "\n",
    "#         # Filter out groups where the number of non- -1 unique otid values is less than min_particles\n",
    "#         def filter_group(group):\n",
    "#             non_negative_unique_otid_count = group[group['otid'] != -1]['otid'].nunique()\n",
    "#             return non_negative_unique_otid_count >= min_particles\n",
    "\n",
    "        #df = grouped.filter(filter_group)\n",
    "        \n",
    "        if do_intersections:\n",
    "            df = self._filter_peak_time(df, do_intersections)\n",
    "            df = self._one_hot_encode(df, 'sector', 6)\n",
    "            df = self._one_hot_encode(df, 'layer', 3)\n",
    "            df = self._rescale_columns(df, ['xo_A','xo_B','xo_C','xe_A','xe_B','xe_C','yo_A','yo_B','yo_C','ye_A','ye_B','ye_C','energy_A', 'energy_B','energy_C','time_A','time_B','time_C', 'centroid_x','centroid_y'],do_intersections)\n",
    "            df = self._reorder_columns(df)\n",
    "        else:\n",
    "            df = self._filter_peak_time(df, do_intersections)\n",
    "            df = self._one_hot_encode(df, 'sector', 6)\n",
    "            df = self._one_hot_encode(df, 'layer', 9)\n",
    "            df = self._rescale_columns(df, ['energy', 'time', 'xo', 'yo', 'zo', 'xe', 'ye', 'ze'],do_intersections)\n",
    "            df = self._delete_columns(df, ['otid', 'event_number', 'status', 'id'])\n",
    "            df = self._reorder_columns(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _filter_peak_time(self, df, do_intersections):\n",
    "        \"\"\"\n",
    "        Filter rows where peak time is outside the range 0-200.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to filter.\n",
    "        do_intersections : bool\n",
    "            If True, preprocess intersection related dataframe\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The filtered DataFrame.\n",
    "        \"\"\"\n",
    "        if do_intersections:\n",
    "            return  df[(df['time_A'] >= 0) & (df['time_A'] <= 200) & (df['time_A'] >= 0) & (df['time_A'] >= 0) &\n",
    "                   (df['time_B'] >= 0) & (df['time_B'] <= 200) & (df['time_B'] >= 0) & (df['time_B'] >= 0) &\n",
    "                   (df['time_C'] >= 0) & (df['time_C'] <= 200) & (df['time_C'] >= 0) & (df['time_C'] >= 0)].copy()\n",
    "        else:\n",
    "            return df[(df['time'] >= 0) & (df['time'] <= 200) & (df['time'] >= 0) & (df['time'] >= 0)].copy()\n",
    "\n",
    "    def _rescale_columns(self, df, columns_to_scale, do_intersections):\n",
    "        \"\"\"\n",
    "        Rescale specified numeric columns between 0-1, with certain groups sharing the same min-max scaling.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to rescale.\n",
    "        columns_to_scale : list of str\n",
    "            List of column names to scale.\n",
    "        do_intersections : bool\n",
    "            If True, preprocess intersection related dataframe\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The DataFrame with scaled columns.\n",
    "        \"\"\"\n",
    "        if do_intersections:\n",
    "            # Group 1: Shared scaling for 'centroid_x', 'centroid_y'\n",
    "            group_1 = ['centroid_x','centroid_y']\n",
    "            group_1_values = df[group_1].values.flatten()  # Flatten to find global min and max\n",
    "            min_val = group_1_values.min()\n",
    "            max_val = group_1_values.max()\n",
    "            df[group_1] = (df[group_1] - min_val) / (max_val - min_val)\n",
    "            \n",
    "            # Group 2: Shared scaling for 'xo_A','xo_B','xo_C','xe_A','xe_B','xe_C','yo_A','yo_B','yo_C','ye_A','ye_B','ye_C'\n",
    "            group_2 = ['xo_A','xo_B','xo_C','xe_A','xe_B','xe_C','yo_A','yo_B','yo_C','ye_A','ye_B','ye_C']\n",
    "            group_2_values = df[group_2].values.flatten()  # Flatten to find global min and max\n",
    "            min_val = group_2_values.min()\n",
    "            max_val = group_2_values.max()\n",
    "            df[group_2] = (df[group_2] - min_val) / (max_val - min_val)\n",
    "            # Other columns to scale individually\n",
    "            other_columns = [col for col in columns_to_scale if col not in group_1 + group_2]\n",
    "        else:\n",
    "            # Group 1: Shared scaling for 'xo', 'yo', 'xe', 'ye'\n",
    "            group_1 = ['xo', 'yo', 'xe', 'ye']\n",
    "            group_1_values = df[group_1].values.flatten()  # Flatten to find global min and max\n",
    "            min_val = group_1_values.min()\n",
    "            max_val = group_1_values.max()\n",
    "            df[group_1] = (df[group_1] - min_val) / (max_val - min_val)\n",
    "\n",
    "            # Group 2: Shared scaling for 'zo' and 'ze'\n",
    "            group_2 = ['zo', 'ze']\n",
    "            group_2_values = df[group_2].values.flatten()  # Flatten to find global min and max\n",
    "            min_val = group_2_values.min()\n",
    "            max_val = group_2_values.max()\n",
    "            df[group_2] = (df[group_2] - min_val) / (max_val - min_val)\n",
    "\n",
    "            # Other columns to scale individually\n",
    "            other_columns = [col for col in columns_to_scale if col not in group_1 + group_2]\n",
    "            \n",
    "        df[other_columns] = self.scaler.fit_transform(df[other_columns])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _delete_columns(self, df, columns_to_delete):\n",
    "        \"\"\"\n",
    "        Delete specified columns from the DataFrame if they exist.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to modify.\n",
    "        columns_to_delete : list of str\n",
    "            List of column names to delete if they exist in the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The DataFrame with specified columns removed.\n",
    "        \"\"\"\n",
    "        return df.drop(columns=[col for col in columns_to_delete if col in df.columns])\n",
    "    \n",
    "    \n",
    "    def _one_hot_encode(self, df, column_name, num_categories):\n",
    "        \"\"\"\n",
    "        One-hot encode a specified column.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to modify.\n",
    "        column_name : str\n",
    "            The name of the column to one-hot encode.\n",
    "        num_categories : int\n",
    "            The number of unique categories in the column.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The DataFrame with one-hot encoded columns.\n",
    "        \"\"\"\n",
    "        for i in range(1, num_categories + 1):\n",
    "            df[f'{column_name}_{i}'] = (df[column_name] == i).astype(int)\n",
    "        df = df.drop(columns=[column_name])\n",
    "        return df    \n",
    "\n",
    "\n",
    "    def _reorder_columns(self, df):\n",
    "        \"\"\"\n",
    "        Reorder the DataFrame columns\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame to reorder.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            The reordered DataFrame.\n",
    "        \"\"\"\n",
    "        # Define the new order of columns\n",
    "        column_order = [\n",
    "            'file_number', 'file_event_number', 'unique_otid', 'mc_pid'\n",
    "        ] + [col for col in df.columns if col not in [\n",
    "            'file_number', 'file_event_number', 'unique_otid', 'mc_pid'\n",
    "        ]]\n",
    "\n",
    "        # Reorder the columns\n",
    "        df = df[column_order]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "class TrainData:\n",
    "    \"\"\"\n",
    "    Class to handle and merge multiple intersection CSV files into a single DataFrame,\n",
    "    ensuring unique mc_index across all files and splitting the data into train and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_files, train_size=0.8, return_tensor=False, K=10, do_intersections=False, remove_background=False, min_particles=1):\n",
    "        \"\"\"\n",
    "        Initialize the TrainData class with a list of CSV files and the desired train/test split.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        csv_files : list[str]\n",
    "            List of paths to the intersection CSV files.\n",
    "        train_size : float\n",
    "            The proportion of the dataset to include in the train split (default is 0.8).\n",
    "        return_tensor : bool\n",
    "            If True, returns the data as tensors instead of DataFrames.\n",
    "        K : int\n",
    "            The number of elements to include in each tensor along the second dimension.\n",
    "        do_intersections : bool\n",
    "            If True, perform intersection analysis after data split.\n",
    "        remove_background : bool\n",
    "            If True, remove rows where the mc_index==-1 (peaks not associated with an MC::Particle)\n",
    "        min_particles : int\n",
    "            Minimum number of particles per event \n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.train_size = train_size\n",
    "        self.return_tensor = return_tensor\n",
    "        self.K = K\n",
    "        self.do_intersections = do_intersections\n",
    "        self.remove_background = remove_background\n",
    "        self.min_particles    = min_particles\n",
    "        self.data = pd.DataFrame()\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "        # Automatically load, merge, preprocess, and split the data upon initialization\n",
    "        self._load_and_merge_csvs()\n",
    "        self._preprocess_data()\n",
    "        self._split_data()\n",
    "\n",
    "\n",
    "    def _load_and_merge_csvs(self):\n",
    "        unique_otid_offset = 0\n",
    "        merged_data = []\n",
    "\n",
    "        for file_number, csv_file in tqdm(enumerate(self.csv_files), total=len(self.csv_files)):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {csv_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Skip empty files\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "\n",
    "            df['file_number'] = file_number\n",
    "            df['file_event_number'] = df['event_number']\n",
    "\n",
    "            # Create the unique_otid column\n",
    "            df['unique_otid'] = df['otid']\n",
    "\n",
    "            # Adjust the otid values for uniqueness across files, skipping -1\n",
    "            non_negative_mask = df['unique_otid'] != -1\n",
    "\n",
    "            # Combine file_event_number, otid, and file_number to ensure uniqueness\n",
    "            df.loc[non_negative_mask, 'unique_otid'] = (\n",
    "                df.loc[non_negative_mask, 'file_event_number'].astype(str) + \"_\" +\n",
    "                df.loc[non_negative_mask, 'otid'].astype(str) + \"_\" +\n",
    "                df.loc[non_negative_mask, 'file_number'].astype(str)\n",
    "            ).astype('category').cat.codes + unique_otid_offset\n",
    "\n",
    "            # Update the offset for the next file, skipping the -1 value\n",
    "            if df.loc[non_negative_mask, 'unique_otid'].max() != -1:\n",
    "                unique_otid_offset = df.loc[non_negative_mask, 'unique_otid'].max() + 1\n",
    "\n",
    "            merged_data.append(df)\n",
    "\n",
    "        self.data = pd.concat(merged_data, ignore_index=True)\n",
    "\n",
    "        if self.data.empty:\n",
    "            raise ValueError(\"No data available after loading and merging CSV files. Please check your input files or filtering criteria.\")\n",
    "\n",
    "        print(f\"Total files processed: {len(self.csv_files)}\")\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        preprocessor = DataPreprocessor()\n",
    "        self.data = preprocessor.preprocess(self.data, self.remove_background, self.do_intersections, self.min_particles)\n",
    "\n",
    "    def _split_data(self):\n",
    "        event_groups = self.data.groupby(['file_number', 'file_event_number'])\n",
    "        # Get a list of unique events\n",
    "        unique_events = event_groups.size().index.tolist()\n",
    "        # Shuffle the events\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(unique_events)\n",
    "\n",
    "        # Calculate the number of events for the training set\n",
    "        train_event_count = int(len(unique_events) * self.train_size)\n",
    "        # Split the events into train and test sets\n",
    "        train_events = unique_events[:train_event_count]\n",
    "        test_events = unique_events[train_event_count:]\n",
    "        # Select data corresponding to the train and test events\n",
    "        self.train_data = self.data[self.data.set_index(['file_number', 'file_event_number']).index.isin(train_events)]\n",
    "        self.test_data = self.data[self.data.set_index(['file_number', 'file_event_number']).index.isin(test_events)]\n",
    "        if self.return_tensor:\n",
    "            self.train_data = self._convert_to_tensor(self.train_data)\n",
    "            self.test_data = self._convert_to_tensor(self.test_data)\n",
    "\n",
    "    def _convert_to_tensor(self, df):\n",
    "        # Add new columns from intersection data if do_intersections is True\n",
    "        if self.do_intersections:\n",
    "            feature_columns = [\n",
    "                'centroid_x', 'centroid_y',\n",
    "                'energy_A', 'energy_B', 'energy_C',\n",
    "                'time_A',   'time_B'  , 'time_C'  ,\n",
    "                'layer_1', 'layer_2', 'layer_3',\n",
    "                'sector_1', 'sector_2', 'sector_3',\n",
    "                 'sector_4', 'sector_5', 'sector_6', \n",
    "                'rec_pid', 'pindex', 'mc_pid',\n",
    "                'xo_A','xo_B','xo_C','xe_A','xe_B','xe_C',\n",
    "                'yo_A','yo_B','yo_C','ye_A','ye_B','ye_C', 'unique_otid',\n",
    "            ]\n",
    "        else:\n",
    "            feature_columns = [\n",
    "                'energy', 'time', 'xo', 'yo', 'zo', 'xe', 'ye', 'ze',\n",
    "                'sector_1', 'sector_2', 'sector_3', 'sector_4',\n",
    "                'sector_5', 'sector_6', 'layer_1', 'layer_2', 'layer_3', 'layer_4',\n",
    "                'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', \n",
    "                'rec_pid', 'pindex', 'mc_pid', 'unique_otid'\n",
    "            ]\n",
    "\n",
    "        tensors = []\n",
    "        grouped = df.groupby(['file_number', 'file_event_number'])\n",
    "        for _, group in tqdm(grouped):\n",
    "            if self.do_intersections:\n",
    "                group = group.sort_values(by='energy_A', ascending=False)\n",
    "            else:\n",
    "                group = group.sort_values(by='energy', ascending=False)\n",
    "            \n",
    "            tensor = group[feature_columns].values[:self.K]\n",
    "\n",
    "            if len(tensor) < self.K:\n",
    "                # Create padding with zeros for all columns except the last one\n",
    "                padding = np.zeros((self.K - len(tensor), len(feature_columns)))\n",
    "                padding[:, -1] = -1  # Set the last column to -1 in the padding\n",
    "\n",
    "                tensor = np.vstack([tensor, padding])\n",
    "\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        return tf.convert_to_tensor(tensors, dtype=tf.float32)\n",
    "\n",
    "    def get_train_data(self, maxN=-1):\n",
    "        return self._get_data(self.train_data, maxN)\n",
    "\n",
    "    def get_test_data(self, maxN=-1):\n",
    "        return self._get_data(self.test_data, maxN)\n",
    "\n",
    "    def _get_data(self, data, maxN):\n",
    "        \"\"\"\n",
    "        Retrieve processed data, potentially limited by maxN.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : np.array\n",
    "            The data array to process.\n",
    "        maxN : int\n",
    "            Maximum number of samples to return. If maxN <= 0, return all data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple of np.array\n",
    "            Returns X, y, and misc data arrays.\n",
    "        \"\"\"\n",
    "        if maxN > 0:\n",
    "            data = data[:maxN]\n",
    "\n",
    "        if self.do_intersections:\n",
    "            X = data[:, :, :17]\n",
    "            y = data[:, :, -1:]\n",
    "            misc = data[:, :, 17:-1]\n",
    "        else:\n",
    "            X = data[:, :, :23]\n",
    "            y = data[:, :, -1:]\n",
    "            misc = data[:, :, 23:-1]\n",
    "\n",
    "            # Cast y to int32\n",
    "            y = tf.cast(y, tf.int32)\n",
    "\n",
    "        return X, y, misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solved-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../projects/test.dis.true.08.21.2024.09.43/training/'\n",
    "all_files = [directory+\"/\"+file for file in os.listdir(directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedicated-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/5 [00:00<?, ?it/s]/scratch/slurm/26616532/ipykernel_1349294/1911675262.py:282: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[non_negative_mask, 'unique_otid'] = (\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files processed: 5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'time_A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/w/hallb-scshelf2102/clas12/users/gmat/venv/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/w/hallb-scshelf2102/clas12/users/gmat/venv/venv/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/w/hallb-scshelf2102/clas12/users/gmat/venv/venv/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time_A'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m param_intersections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mreturn_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdo_intersections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_intersections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mremove_background\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_X, train_y, train_misc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_train_data()\n\u001b[1;32m     11\u001b[0m test_X, test_y, test_misc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_test_data()\n",
      "Cell \u001b[0;32mIn[7], line 253\u001b[0m, in \u001b[0;36mTrainData.__init__\u001b[0;34m(self, csv_files, train_size, return_tensor, K, do_intersections, remove_background, min_particles)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Automatically load, merge, preprocess, and split the data upon initialization\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_and_merge_csvs()\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_data()\n",
      "Cell \u001b[0;32mIn[7], line 303\u001b[0m, in \u001b[0;36mTrainData._preprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    302\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m DataPreprocessor()\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_intersections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_particles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mDataPreprocessor.preprocess\u001b[0;34m(self, df, remove_background, do_intersections, min_particles)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#         # Group by file_number and file_event_number\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#         grouped = df.groupby(['file_number', 'file_event_number'])\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m#df = grouped.filter(filter_group)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m do_intersections:\n\u001b[0;32m---> 50\u001b[0m             df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_peak_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_intersections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m             df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_hot_encode(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     52\u001b[0m             df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_hot_encode(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mDataPreprocessor._filter_peak_time\u001b[0;34m(self, df, do_intersections)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03mFilter rows where peak time is outside the range 0-200.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    The filtered DataFrame.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_intersections:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  df[(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_A\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     82\u001b[0m            (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_B\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_B\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_B\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_B\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     83\u001b[0m            (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_C\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_C\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_C\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_C\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/w/hallb-scshelf2102/clas12/users/gmat/venv/venv/lib/python3.8/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/w/hallb-scshelf2102/clas12/users/gmat/venv/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time_A'"
     ]
    }
   ],
   "source": [
    "param_intersections = True\n",
    "\n",
    "data = TrainData(csv_files = all_files[0:5],\n",
    "                 train_size = 0.8,\n",
    "                 return_tensor = True,\n",
    "                 K=20,\n",
    "                 do_intersections=param_intersections,\n",
    "                 remove_background=False)\n",
    "\n",
    "train_X, train_y, train_misc = data.get_train_data()\n",
    "test_X, test_y, test_misc = data.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "coordinated-guard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([                'energy', 'time', 'xo', 'yo', 'zo', 'xe', 'ye', 'ze',\n",
    "                'sector_1', 'sector_2', 'sector_3', 'sector_4',\n",
    "                'sector_5', 'sector_6', 'layer_1', 'layer_2', 'layer_3', 'layer_4',\n",
    "                'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "pointed-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1071, 20, 1), dtype=int32, numpy=\n",
       "array([[[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]],\n",
       "\n",
       "       [[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]],\n",
       "\n",
       "       [[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]],\n",
       "\n",
       "       [[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]],\n",
       "\n",
       "       [[-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        ...,\n",
       "        [-2147483648],\n",
       "        [-2147483648],\n",
       "        [-2147483648]]], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-reproduction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_python38",
   "language": "python",
   "name": "venv_python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
